{% set name = "spacy-transformers" %}
{% set module_name = "spacy_transformers" %}
{% set version = "1.3.5" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  # To apply fix-transformers-upper-bound.patch we need the same setup.cfg as on the GitHub but this file is different on PyPI.
  # When the patch won't be required then the PyPI tarball can be back again
  #url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name.replace('-', '_') }}-{{ version }}.tar.gz
  url: https://github.com/explosion/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz
  sha256: 2da7ad6706590253d759fce6bd01640a9fb595d586ca7ae47eb774c274ee73a6
  patches:
    # This patch might not be needed at the time of the next release.
    - fix-transformers-upper-bound.patch

build:
  number: 0
  # spacy isn't available on s390x.
  skip: True  # [s390x]
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation --ignore-installed --no-cache-dir -vv

requirements:
  build:
    - {{ compiler('cxx') }}
    - m2-patch  # [win]
    - patch     # [not win]
  host:
    - python
    - cython >=0.25
    - numpy {{ numpy }}
    - pip
    - setuptools
    - wheel
  run:
    - python
    - {{ pin_compatible('numpy') }}
    - spacy >=3.5.0,<4.1.0
    # Anaconda applies a patch and sets the transformers upper bound to <4.42.0 because of compatibility with 'tokenisers' versions
    - transformers >=3.4.0,<4.42.0
    - pytorch >=1.8.0
    - srsly >=2.4.0,<3.0.0
    - spacy-alignments >=0.7.2,<1.0.0
  run_constrained:
    pydantic >=1.0.0,<2.0.0

test:
  requires:
    - pip
    - pytest >=5.2.0
    # test_pipeline_component.py::test_transformer_sentencepiece_IO:
    # XLMRobertaTokenizer requires the SentencePiece library
    - sentencepiece
  imports:
    - {{ module_name }}
  commands:
    - pip check
    # tests take too long (hours) on ppc64le and aarch64
    - python -m pytest --tb=native --pyargs {{ module_name }}  # [not (linux and aarch64)]

about:
  home: https://spacy.io
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Use pretrained transformers like BERT, XLNet and GPT-2 in spaCy
  description: |
    This package provides spaCy components and architectures to use transformer
    models via Hugging Face's transformers in spaCy. The result is convenient
    access to state-of-the-art transformer architectures, such as BERT, GPT-2,
    XLNet, etc.
  doc_url: https://github.com/explosion/spacy-transformers
  dev_url: https://github.com/explosion/spacy-transformers

extra:
  recipe-maintainers:
    - honnibal
    - ines
    - adrianeboyd
