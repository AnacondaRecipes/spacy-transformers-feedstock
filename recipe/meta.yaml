{% set name = "spacy-transformers" %}
{% set module_name = "spacy_transformers" %}
{% set version = "1.1.5" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 9f16e69c5c87a6d6dee4ceeb422d84086a01a7152ebad742b58db4787b060cf2

build:
  noarch: python
  number: 1
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv

requirements:
  host:
    - python
    - pip
    - setuptools
    - wheel
  run:
    - python >=3.6
    - spacy >=3.1.3,<4.0.0
    - transformers >=3.4.0,<4.18.0
    - pytorch >=1.6.0
    - spacy >=3.1.3,<4.0.0
    - spacy-alignments >=0.7.2,<1.0.0
    - srsly >=2.4.0,<3.0.0
    # We can safely extend transformers to 4.18.0
    # see https://github.com/explosion/spacy-transformers/commit/d749a38d77fff3a47565ddcc58e3a41f5ffd6e05
    - transformers >=3.4.0,<4.19.0

test:
  imports:
    - {{ module_name }}
  requires:
    - pip
    #- pytest
  commands:
    - pip check
    #- python -m pytest --tb=native --pyargs {{ module_name }}

about:
  home: https://spacy.io
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Use pretrained transformers like BERT, XLNet and GPT-2 in spaCy
  description: |
    This package provides spaCy components and architectures to use transformer
    models via Hugging Face's transformers in spaCy. The result is convenient
    access to state-of-the-art transformer architectures, such as BERT, GPT-2,
    XLNet, etc.
  doc_url: https://spacy.io/usage/embeddings-transformers
  dev_url: https://github.com/explosion/spacy-transformers

extra:
  recipe-maintainers:
    - honnibal
    - ines
    - adrianeboyd
